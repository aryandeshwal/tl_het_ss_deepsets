{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tkwargs = {\"dtype\": torch.float64, \"device\": device}\n",
    "torch.set_default_dtype(torch.float64)\n",
    "print(f\"device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpob_handler import HPOBHandler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from dkl_gp import GPModelDKL\n",
    "from botorch.utils.transforms import normalize, unnormalize, standardize\n",
    "from gpytorch.mlls import PredictiveLogLikelihood, ExactMarginalLogLikelihood, VariationalELBO\n",
    "import gpytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data from HPO-B library https://github.com/releaunifreiburg/HPO-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpob_hdlr = HPOBHandler(root_dir=\"hpob-data/\", mode=\"v3\")\n",
    "# search_space_id =  hpob_hdlr.get_search_spaces()[0]\n",
    "# dataset_id = hpob_hdlr.get_datasets(search_space_id)\n",
    "\n",
    "# xgboost ids in HPO-B\n",
    "search_space_ids = ['5906', '5971', '6767']\n",
    "# 3 random datasets\n",
    "dataset_ids = ['9957', '145862', '3891']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor_and_standardize(X, y, bounds=None, Y_mean=None, Y_std=None):\n",
    "    X = torch.tensor(X, **tkwargs)\n",
    "    y = torch.tensor(y, **tkwargs).squeeze()\n",
    "    print(X.shape, y.shape)\n",
    "    if bounds is None:\n",
    "        bounds = (torch.stack([torch.min(X, dim=0)[0], torch.max(X, dim=0)[0] + 1e-8]))\n",
    "    X = normalize(X, bounds=bounds)\n",
    "    if Y_mean is None:\n",
    "        stddim = -1 if y.dim() < 2 else -2\n",
    "        Y_std = y.std(dim=stddim, keepdim=True)\n",
    "        Y_std = Y_std.where(Y_std >= 1e-9, torch.full_like(Y_std, 1.0))\n",
    "        Y_mean = y.mean(dim=stddim, keepdim=True)\n",
    "    y = (y - Y_mean) / Y_std\n",
    "    print(torch.any(torch.isnan(X)))\n",
    "    print(torch.any(torch.isnan(y)))\n",
    "    return X, y, bounds, Y_mean, Y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_by_search_id(search_space_id, dataset_index = 0):\n",
    "    dataset_keys = list(hpob_hdlr.meta_test_data[search_space_id].keys())\n",
    "    key = dataset_keys[dataset_index]\n",
    "    X = hpob_hdlr.meta_test_data[search_space_id][key][\"X\"]\n",
    "    y = hpob_hdlr.meta_test_data[search_space_id][key][\"y\"]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data_by_search_id(search_space_id):\n",
    "    all_Xs, all_ys = [], []\n",
    "    dataset_keys = list(hpob_hdlr.meta_train_data[search_space_id].keys())\n",
    "    for key in dataset_keys:\n",
    "        all_Xs.extend(hpob_hdlr.meta_train_data[search_space_id][key][\"X\"])\n",
    "        all_ys.extend(hpob_hdlr.meta_train_data[search_space_id][key][\"y\"])\n",
    "    return all_Xs, all_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_data_by_search_id(search_space_id):\n",
    "    all_Xs, all_ys = [], []\n",
    "    dataset_keys = list(hpob_hdlr.meta_validation_data[search_space_id].keys())\n",
    "    for key in dataset_keys:\n",
    "        all_Xs.extend(hpob_hdlr.meta_validation_data[search_space_id][key][\"X\"])\n",
    "        all_ys.extend(hpob_hdlr.meta_validation_data[search_space_id][key][\"y\"])\n",
    "    return all_Xs, all_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covering_support_query_sets(X, y, task_idx, support_size=10, query_size=22, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    rand_indices = np.random.permutation(len(X))\n",
    "    X = X[rand_indices]\n",
    "    y = y[rand_indices]\n",
    "    batch_size = support_size + query_size\n",
    "    num_batches = len(X) // batch_size\n",
    "    data_batches = []\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        support_idx = start_idx + support_size\n",
    "        end_idx = (i + 1) * batch_size    \n",
    "        sup_x = X[start_idx:support_idx]\n",
    "        sup_y = y[start_idx:support_idx]\n",
    "        que_x = X[support_idx:end_idx]\n",
    "        que_y = y[support_idx:end_idx]\n",
    "        assert sup_x.shape[0] == support_size\n",
    "        assert sup_y.shape[0] == support_size\n",
    "        assert que_x.shape[0] == query_size\n",
    "        assert que_y.shape[0] == query_size\n",
    "        data_batches.append([sup_x, sup_y.unsqueeze(-1), que_x, que_y.unsqueeze(-1), task_idx])\n",
    "    return data_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episodes_data(\n",
    "        all_Xs, \n",
    "        all_ys, \n",
    "        n_episodes=1000,\n",
    "        task_idx=0, \n",
    "        support_size=10, \n",
    "        query_size=22\n",
    "    ):\n",
    "    \"\"\"This method collects episode level data.\n",
    "    \"\"\"\n",
    "    all_points = []\n",
    "    n_episodes_per_sweep = int(len(all_Xs)/(support_size + query_size))\n",
    "    for uniq_seed in range((n_episodes // n_episodes_per_sweep) + 1):\n",
    "        batch_data = get_covering_support_query_sets(all_Xs, \n",
    "                                                    all_ys, \n",
    "                                                    task_idx=task_idx, \n",
    "                                                    support_size=support_size, \n",
    "                                                    query_size=query_size, \n",
    "                                                    seed=uniq_seed)\n",
    "        # print(len(batch_data))\n",
    "        all_points.extend(batch_data)\n",
    "    return all_points[:n_episodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def single_update_step(\n",
    "#     model,\n",
    "#     mll,\n",
    "#     optimizer,\n",
    "#     sup_x,\n",
    "#     sup_y,\n",
    "#     que_x,\n",
    "#     que_y,\n",
    "# ):\n",
    "#     \"\"\"This method trains the model one episode at a time.\n",
    "#     \"\"\"\n",
    "#     model = model.train() \n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(sup_x.cuda(), sup_y.cuda(), que_x.cuda())\n",
    "#     loss = -mll(output, que_y.squeeze().cuda())\n",
    "#     loss.backward()\n",
    "#     # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#     optimizer.step()\n",
    "#     model = model.eval()\n",
    "\n",
    "#     return model, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_update_step(\n",
    "    model,\n",
    "    mll,\n",
    "    optimizer,\n",
    "    batch_data,\n",
    "):\n",
    "    \"\"\"Training loop operating on a batch of episodes.\n",
    "    \"\"\"\n",
    "    model = model.train() \n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0.0\n",
    "    for i in range(len(batch_data)):\n",
    "        sup_x, sup_y, que_x, que_y, task_idx = batch_data[i]\n",
    "        output = model(sup_x.cuda(), sup_y.cuda(), que_x.cuda())\n",
    "        loss = -mll(output, que_y.squeeze().cuda())\n",
    "        total_loss = total_loss + loss\n",
    "    total_loss = (total_loss / len(batch_data))\n",
    "    total_loss.backward()\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    model = model.eval()\n",
    "    return model, total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_loss(model,\n",
    "                        mll,\n",
    "                        valid_data):\n",
    "    model = model.eval() \n",
    "    total_loss = 0.0\n",
    "    for i in range(len(valid_data)):\n",
    "        sup_x, sup_y, que_x, que_y, task_idx = valid_data[i]\n",
    "        with torch.no_grad():\n",
    "            output = model(sup_x.cuda(), sup_y.cuda(), que_x.cuda())\n",
    "            loss = -mll(output, que_y.squeeze().cuda())\n",
    "        total_loss = total_loss + loss.item()\n",
    "    return total_loss / len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquisition function needs to be defined manually because \n",
    "# each pass to the model requires (sup_x, sup_y, que_x)\n",
    "from botorch.acquisition.analytic import _scaled_improvement, _ei_helper\n",
    "def ei_acqf(model, inputs, best_f):\n",
    "    dist = model.posterior(*inputs)\n",
    "    mean = dist.mean\n",
    "    sigma = dist.variance ** (1/2)\n",
    "    u = _scaled_improvement(mean, sigma, best_f, maximize=True)\n",
    "    return sigma * _ei_helper(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data setup\n",
    "all_train_Xs, all_train_ys = get_train_data_by_search_id(search_space_ids[0])\n",
    "print(len(all_train_Xs), len(all_train_ys))\n",
    "(\n",
    "    all_train_Xs, \n",
    "    all_train_ys, \n",
    "    train_bounds, \n",
    "    train_Y_mean, \n",
    "    train_Y_std\n",
    ")   = convert_tensor_and_standardize(all_train_Xs, all_train_ys)\n",
    "episodes_data = get_episodes_data(all_train_Xs, all_train_ys, n_episodes=2000)\n",
    "print(len(episodes_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data setup\n",
    "all_valid_Xs, all_valid_ys = get_validation_data_by_search_id(search_space_ids[0])\n",
    "all_valid_Xs, all_valid_ys, _, _, _ = convert_tensor_and_standardize(all_valid_Xs, \n",
    "                                                            all_valid_ys, \n",
    "                                                            bounds=train_bounds,\n",
    "                                                            Y_mean=train_Y_mean,\n",
    "                                                            Y_std=train_Y_std\n",
    "                                                            )\n",
    "validation_episodes_data = get_covering_support_query_sets(\n",
    "                                                    all_valid_Xs, \n",
    "                                                    all_valid_ys, \n",
    "                                                    task_idx=0, \n",
    "                                                    support_size=22, \n",
    "                                                    query_size=10, \n",
    "                                                    seed=0\n",
    "                            )\n",
    "# sup_x, sup_y, que_x, que_y, task_idx = episodes_data[0]\n",
    "# for episode in validation_episodes_data:\n",
    "#     episode[0] = sup_x\n",
    "#     episode[1] = sup_y\n",
    "print(len(validation_episodes_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data setup\n",
    "all_test_Xs, all_test_ys = get_test_data_by_search_id(search_space_ids[0])\n",
    "all_test_Xs, all_test_ys, _, _, _ = convert_tensor_and_standardize(all_test_Xs, all_test_ys)\n",
    "test_episodes_data = get_covering_support_query_sets(all_test_Xs, \n",
    "                                                    all_test_ys, \n",
    "                                                    task_idx=1, \n",
    "                                                    support_size=10, \n",
    "                                                    query_size=10, \n",
    "                                                    seed=0)\n",
    "print(len(test_episodes_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes_data.append(test_episodes_data[0])\n",
    "# episodes_data.append(validation_episodes_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters to choose for the model\n",
    "- Episode parameters:\n",
    "    - support_size: 10\n",
    "    - query_size: 22\n",
    "    - n_episodes: 1000\n",
    "- Batch size (at the episode level): 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_pts_size = 20\n",
    "np.random.seed(0)\n",
    "rand_indices = np.random.permutation(len(episodes_data))[:ind_pts_size]\n",
    "inducing_points = [episodes_data[idx] for idx in rand_indices]\n",
    "print(len(inducing_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood =  gpytorch.likelihoods.GaussianLikelihood().cuda() \n",
    "gp_model = GPModelDKL(inducing_points_set=inducing_points, \n",
    "                      likelihood=likelihood, \n",
    "                      network_dims=(64, 64),\n",
    "                      learn_inducing_locations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rte = 0.001\n",
    "optimizer = torch.optim.Adam([{'params': gp_model.parameters(), 'lr': learning_rte} ], lr=learning_rte)\n",
    "mll = PredictiveLogLikelihood(gp_model.likelihood, gp_model, num_data=len(episodes_data) ** 22).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = []\n",
    "validation_losses = []\n",
    "batch_size = 32\n",
    "for n_epochs in range(500):\n",
    "    for i in range(0, len(episodes_data) // batch_size, batch_size):\n",
    "        gp_model, loss = batch_update_step(gp_model, \n",
    "                                           mll, \n",
    "                                           optimizer, \n",
    "                                           episodes_data[i * batch_size: (i+1)*batch_size] \n",
    "                        )\n",
    "        validation_losses.append(get_validation_loss(gp_model, mll, validation_episodes_data))\n",
    "        all_losses.append(loss)\n",
    "        if len(all_losses) % 50 == 0 and len(all_losses) != 0:\n",
    "            # all_losses = np.array(all_losses)\n",
    "            plt.plot(all_losses[10:], label='training loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            # plt.plot(validation_losses, label='validation loss')\n",
    "            # plt.legend()\n",
    "            # plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bonet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
